{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run D:/Code/py_code/Convolutional-Neural-Network/experiment/widi/helper_func.py\n",
    "\n",
    "from helper_func import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, make_confusion_matrix, load_and_prep_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(\"D:\\Code\\py_code\\Convolutional-Neural-Network\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"D:\\Code\\py_code\\Convolutional-Neural-Network\\data\\train\"\n",
    "test_dir = r\"D:\\Code\\py_code\\Convolutional-Neural-Network\\data\\test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "preprocessing_function = tf.keras.applications.densenet.preprocess_input\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_datagen = ImageDataGenerator(validation_split = 0.2,\n",
    "                                   rotation_range = 0.2,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   rescale = 1./255,\n",
    "                                   preprocessing_function=preprocessing_function\n",
    "                                  )\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function,\n",
    "                                  rescale = 1./255.)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                    target_size = IMG_SIZE,\n",
    "                                                    batch_size = 32,\n",
    "                                                    shuffle  = True ,\n",
    "                                                    color_mode = \"rgb\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    subset = \"training\",\n",
    "                                                    seed = 42\n",
    "                                                   )\n",
    "\n",
    "validation_data = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                         target_size = IMG_SIZE,\n",
    "                                                         batch_size = 32,\n",
    "                                                         shuffle  = True ,\n",
    "                                                         color_mode = \"rgb\",\n",
    "                                                         class_mode = \"categorical\",\n",
    "                                                         subset = \"validation\",\n",
    "                                                         seed = 42\n",
    "                                                        )\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                   target_size = IMG_SIZE,\n",
    "                                                   batch_size = 32,\n",
    "                                                    shuffle  = False ,\n",
    "                                                    color_mode = \"rgb\",\n",
    "                                                    class_mode = \"categorical\",\n",
    "                                                    seed = 42\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# IMG_SIZE = (224, 224)\n",
    "\n",
    "# train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "#                                                                 color_mode = \"rgb\",\n",
    "#                                                                 label_mode=\"categorical\",\n",
    "#                                                                 image_size=IMG_SIZE)\n",
    "                                                                                \n",
    "# test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "#                                                                 label_mode=\"categorical\",\n",
    "#                                                                 color_mode = \"rgb\",\n",
    "#                                                                 image_size=IMG_SIZE,\n",
    "#                                                                 shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "data_dir = pathlib.Path(train_dir) \n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) \n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create checkpoint callback to save model for later use\n",
    "# checkpoint_path = \"emotion_data_model_checkpoint\"\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "#                                                          save_weights_only=True, \n",
    "#                                                          monitor=\"val_accuracy\", \n",
    "#                                                          save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the required modules for model creation\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# data_augmentation = Sequential([\n",
    "#   layers.RandomFlip(\"horizontal\"),\n",
    "#   layers.RandomZoom(0.2),\n",
    "#   layers.RandomHeight(0.2),\n",
    "#   layers.RandomWidth(0.2),\n",
    "#   preprocessing.Rescaling(1./255)\n",
    "# ], name =\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Setup model architecture with trainable top layers\n",
    "inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "base_model = tf.keras.applications.inception_v3.InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))(inputs)\n",
    "# x = data_augmentation(inputs)\n",
    "# x = base_model(x, training=False) \n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling\")(base_model)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\", name=\"output_layer\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                         patience=2,\n",
    "                                                         verbose= 1 ,\n",
    "                                                         restore_best_weights=True\n",
    "                                                        )\n",
    "\n",
    "# Fit\n",
    "history_all_classes_10_percent = model.fit(train_data,\n",
    "                                           epochs=5,\n",
    "                                           validation_data=validation_data,\n",
    "                                           validation_steps=int(0.15 * len(validation_data)), \n",
    "                                           callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_feature_extraction_model = model.evaluate(test_data)\n",
    "results_feature_extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_all_classes_10_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all of the layers in the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Refreeze every layer except for the last 5\n",
    "for layer in base_model.layers[:-5]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4), # 10x lower learning rate than default\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What layers in the model are trainable?\n",
    "for layer in model.layers:\n",
    "  print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which layers are trainable\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10 # model has already done 5 epochs, this is the total number of epochs we're after (5+5=10)\n",
    "\n",
    "history_all_classes_10_percent_fine_tune = model.fit(train_data,\n",
    "                                                     epochs=fine_tune_epochs,\n",
    "                                                     validation_data=test_data,\n",
    "                                                     validation_steps=int(0.15 * len(test_data)),\n",
    "                                                     initial_epoch=history_all_classes_10_percent.epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model on the whole test dataset\n",
    "results_all_classes_10_percent_fine_tune = model.evaluate(test_data)\n",
    "results_all_classes_10_percent_fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_historys(original_history=history_all_classes_10_percent,\n",
    "                 new_history=history_all_classes_10_percent_fine_tune,\n",
    "                 initial_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict(test_data, verbose=1)\n",
    "pred_classes = pred_probs.argmax(axis=1)\n",
    "pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This might take a minute or so due to unravelling 790 batches\n",
    "y_labels = []\n",
    "for images, labels in test_data.unbatch(): # unbatch the test data and get images and labels\n",
    "  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)\n",
    "y_labels[:10] # check what they look like (unshuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy score by comparing predicted classes to ground truth labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "sklearn_accuracy = accuracy_score(y_labels, pred_classes)\n",
    "sklearn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names\n",
    "class_names = test_data.class_names\n",
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix with all 25250 predictions, ground truth labels and 101 classes\n",
    "make_confusion_matrix(y_true=y_labels,\n",
    "                      y_pred=pred_classes,\n",
    "                      classes=class_names,\n",
    "                      figsize=(20, 20),\n",
    "                      text_size=20,\n",
    "                      norm=False, \n",
    "                      savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_labels, pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of the classification report\n",
    "classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\n",
    "classification_report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary\n",
    "class_f1_scores = {}\n",
    "# Loop through classification report items\n",
    "for k, v in classification_report_dict.items():\n",
    "  if k == \"accuracy\": # stop once we get to accuracy key\n",
    "    break\n",
    "  else:\n",
    "    # Append class names and f1-scores to new dictionary\n",
    "    class_f1_scores[class_names[int(k)]] = v[\"f1-score\"]\n",
    "class_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn f1-scores into dataframe for visualization\n",
    "import pandas as pd\n",
    "f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n",
    "                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\n",
    "f1_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\n",
    "ax.set_yticks(range(len(f1_scores)))\n",
    "ax.set_yticklabels(list(f1_scores[\"class_name\"]))\n",
    "ax.set_xlabel(\"f1-score\")\n",
    "ax.set_title(\"F1-Scores for 10 Different Classes\")\n",
    "ax.invert_yaxis(); # reverse the order\n",
    "\n",
    "def autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html\n",
    "  \"\"\"\n",
    "  Attach a text label above each bar displaying its height (it's value).\n",
    "  \"\"\"\n",
    "  for rect in rects:\n",
    "    width = rect.get_width()\n",
    "    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,\n",
    "            f\"{width:.2f}\",\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "autolabel(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preds on a series of random images\n",
    "import os\n",
    "import random\n",
    "\n",
    "plt.figure(figsize=(17, 10))\n",
    "for i in range(3):\n",
    "  # Choose a random image from a random class \n",
    "  class_name = random.choice(class_names)\n",
    "  filename = random.choice(os.listdir(test_dir + \"/\" + class_name))\n",
    "  filepath = test_dir + \"/\" + class_name + \"/\" + filename\n",
    "\n",
    "  # Load the image and make predictions\n",
    "  img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions\n",
    "  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n",
    "  pred_class = class_names[pred_prob.argmax()] # find the predicted class \n",
    "\n",
    "  # Plot the image(s)\n",
    "  plt.subplot(1, 3, i+1)\n",
    "  plt.imshow(img/255.)\n",
    "  if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n",
    "    title_color = \"g\"\n",
    "  else:\n",
    "    title_color = \"r\"\n",
    "  plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)\n",
    "  plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get most Wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the filenames of all of our test data\n",
    "filepaths = []\n",
    "for filepath in test_data.list_files(r\"D:\\Code\\py_code\\Convolutional-Neural-Network\\data\\test\\*\\*.jpg\", \n",
    "                                     shuffle=False):\n",
    "  filepaths.append(filepath.numpy())\n",
    "filepaths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a dataframe out of current prediction data for analysis\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame({\"img_path\": filepaths,\n",
    "                        \"y_true\": y_labels,\n",
    "                        \"y_pred\": pred_classes,\n",
    "                        \"pred_conf\": pred_probs.max(axis=1), # get the maximum prediction probability value\n",
    "                        \"y_true_classname\": [class_names[i] for i in y_labels],\n",
    "                        \"y_pred_classname\": [class_names[i] for i in pred_classes]}) \n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Is the prediction correct?\n",
    "pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get the top 100 wrong examples\n",
    "top_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False)[:100]\n",
    "top_100_wrong.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize some of the most wrong examples\n",
    "images_to_view = 9\n",
    "start_index = 10 # change the start index to view more\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()): \n",
    "  plt.subplot(3, 3, i+1)\n",
    "  img = load_and_prep_image(row[1], scale=True)\n",
    "  _, _, _, _, pred_prob, y_true, y_pred, _ = row # only interested in a few parameters of each row\n",
    "  plt.imshow(img)\n",
    "  plt.title(f\"actual: {y_true}, pred: {y_pred} \\nprob: {pred_prob:.2f}\")\n",
    "  plt.axis(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
